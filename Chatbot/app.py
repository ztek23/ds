import streamlit as st
import os
from groq import Groq
import random
from langchain.chains import ConversationChain, LLMChain
from langchain_core.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder)
from langchain_core.messages import SystemMessage
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_groq import ChatGroq 
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

def main():
    # This is the entry point of the application, its sets a Groq client with streamlit interface for chat interaction

    # get Groq API key 
    load_dotenv()
    groq_api_key = os.getenv('GROQ_API_KEY')
    
    #display the GROQ logo 
    spacer, col = st.columns([5, 1])
    with col:
        st.image('groqcloud_darkmode.png')

    # Title and greeting message of the streamlit application
    st.title("Chat with Groq")
    st.write("Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!")

    # Add customization options to the sidebar
    st.sidebar.title('Customization')
    system_prompt = st.sidebar.text_input("System prompt:")
    model = st.sidebar.selectbox(
        'Choose a model',
        ['llama3-8b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it']
    )
    conversational_memory_length = st.sidebar.slider('Conversational memory length:', 1, 10, value = 5)

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)

    user_question = st.text_input("Ask a question:")
    
    # Patient state variable 
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history=[]
    else:
        for message in st.session_state.chat_history:
            memory.save_context(
                {'input':message['human']},
                {'output':message['AI']}
                )

    # Initialize Groq lang chain chat objet conversation
    groq_chat = ChatGroq(
        groq_api_key = groq_api_key,
        model_name = model
    )

    # If the user has asked a question
    if user_question:
        # Construct the chat prompt using various components
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content = system_prompt
                ), # This is the persistent system prompt that is included in the start of the chat

                MessagesPlaceholder(
                    variable_name = "chat_history"
                ), # This placeholder will be replaced by actual chat history during the conversation

                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ), # This template is where the users current input will be injected in to the prompt
            ]
        )

        # Create a conversation chain using the LangChain LLM (Language Learning Model)
        conversation = LLMChain(
            llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
            prompt=prompt,  # The constructed prompt template.
            verbose=True,   # Enables verbose output, which can be useful for debugging.
            memory=memory,  # The conversational memory object that stores and manages the conversation history.
        )
        
        # The chatbot's answer is generated by sending the full prompt to the Groq API.
        response = conversation.predict(human_input=user_question)
        message = {'human':user_question,'AI':response}
        st.session_state.chat_history.append(message)
        st.write("Chatbot:", response)

if __name__ == "__main__":
    main()